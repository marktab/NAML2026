{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbda4acd",
   "metadata": {},
   "source": [
    "# Demo 1 — \"Living Scenario Brief\"\n",
    "## Adaptive Situational Awareness Under Uncertainty\n",
    "\n",
    "> **Responsible-AI Scope Statement:** This demo uses **fully synthetic** scenario data—fictional actors, place-names, and intelligence reports—to illustrate multi-agent orchestration patterns. No output constitutes real intelligence, operational guidance, or doctrinal authority. A human decision-maker retains full authority over any real-world action.\n",
    "\n",
    "**OODA Phase: Observe / Orient**\n",
    "\n",
    "**Purpose:** A multi-agent scenario-learning loop that maintains a \"living world state\" — a structured JSON of events, actors, locations, and uncertainties — and updates it each turn as new simulated intelligence arrives. Each cycle produces a structured SITREP explaining *what* the current situation is and *how and why* it changed.\n",
    "\n",
    "**Audience:** Warfighters, operational planners, and C2 staff familiar with the intelligence cycle.\n",
    "**Primary outcome:** The audience sees how multi-agent orchestration automates intelligence fusion, assessment tracking, and briefing generation — the OODA Observe/Orient phases — while keeping the commander in the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c16e04",
   "metadata": {},
   "source": [
    "## What It Illustrates (Multi-Agent)\n",
    "\n",
    "| Agent | Role | OODA Phase |\n",
    "|-------|------|------------|\n",
    "| **Scenario Orchestrator** | Maintains authoritative world-state JSON; integrates new intel each turn | Foundation |\n",
    "| **ISR / Intel Fusion** | Fuses multi-INT reports; flags contradictions, corroborations, and gaps | Observe |\n",
    "| **Assessment Agent** | Updates threat/risk ratings with Bayesian-inspired confidence tracking | Orient |\n",
    "| **Briefing / Explainer** | Produces commander-ready SITREP in military format with evidence citations | Orient → Brief |\n",
    "| **Commander (Human-in-the-Loop)** | Asks clarifying questions, challenges assessments, injects CCIRs | Decide |\n",
    "\n",
    "**Architecture:** AutoGen 0.7 `RoundRobinGroupChat` with four `AssistantAgent`s sharing one `model_client`, streamed via `Console`.\n",
    "\n",
    "**Success criteria:** After 3 turns of escalating intelligence, the system produces increasingly detailed SITREPs that track confidence changes, cite evidence, identify information gaps, and respond to commander queries — all visible in the agent conversation flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df84207",
   "metadata": {},
   "source": [
    "## Azure Technologies Used in This Demo\n",
    "\n",
    "This demo relies on several Azure services working together. If you're new to Azure, here's a quick guide to each technology and how it fits in.\n",
    "\n",
    "### Azure AI Foundry (formerly Azure AI Studio)\n",
    "\n",
    "[Azure AI Foundry](https://learn.microsoft.com/azure/ai-foundry/what-is-ai-foundry) is Microsoft's unified platform for building, evaluating, and deploying generative AI applications. It provides a **project workspace** where you can:\n",
    "\n",
    "- Access multiple large language models (LLMs) from a single endpoint — including GPT-4o, Mistral, Phi, and more\n",
    "- Manage prompt engineering, evaluations, and deployments\n",
    "- Monitor usage, cost, and safety metrics\n",
    "\n",
    "In this demo, Azure AI Foundry hosts the LLM that powers all four agents. The notebook connects via an **inference endpoint** (a URL) and an **API key** (a credential), both stored securely in Azure Key Vault.\n",
    "\n",
    "> **Learn more:** [Get started with Azure AI Foundry](https://learn.microsoft.com/azure/ai-foundry/quickstarts/get-started-playground) | [Azure AI Foundry SDKs](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/sdk-overview)\n",
    "\n",
    "---\n",
    "\n",
    "### Azure Key Vault\n",
    "\n",
    "[Azure Key Vault](https://learn.microsoft.com/azure/key-vault/general/overview) is a cloud service for securely storing and managing **secrets** (API keys, passwords, certificates, and cryptographic keys). Instead of pasting credentials directly into code — which risks accidental exposure through version control — Key Vault provides:\n",
    "\n",
    "- **Centralized secret management** with fine-grained access control (RBAC)\n",
    "- **Audit logging** of every secret access via Azure Monitor\n",
    "- **Automatic rotation and expiration** policies\n",
    "- **FIPS 140-2 validated** hardware-backed storage (HSM tier available)\n",
    "\n",
    "This project stores the Azure AI Foundry endpoint URL and API key as Key Vault secrets. The code retrieves them at runtime using `DefaultAzureCredential` — so **no credential ever appears in the codebase**.\n",
    "\n",
    "> **Learn more:** [About Azure Key Vault](https://learn.microsoft.com/azure/key-vault/general/overview) | [Quickstart: Set and retrieve a secret](https://learn.microsoft.com/azure/key-vault/secrets/quick-create-portal)\n",
    "\n",
    "---\n",
    "\n",
    "### Azure Identity & `DefaultAzureCredential`\n",
    "\n",
    "The [`azure-identity`](https://learn.microsoft.com/python/api/overview/azure/identity-readme) library provides a unified way to authenticate with Azure services. The star of the library is **`DefaultAzureCredential`**, which tries a chain of authentication methods automatically:\n",
    "\n",
    "1. **Environment variables** — checks for `AZURE_CLIENT_ID`, `AZURE_TENANT_ID`, `AZURE_CLIENT_SECRET`\n",
    "2. **Managed Identity** — when running on Azure (VMs, App Service, Azure ML compute), uses the resource's built-in identity with no credentials needed\n",
    "3. **Azure CLI** — uses your `az login` session during local development\n",
    "4. **Visual Studio Code** — uses your signed-in Azure account in VS Code\n",
    "\n",
    "This \"just works\" pattern means the same code runs locally (using your `az login`) and in Azure (using Managed Identity) without any code changes.\n",
    "\n",
    "> **Learn more:** [DefaultAzureCredential overview](https://learn.microsoft.com/python/api/overview/azure/identity-readme#defaultazurecredential) | [Azure Identity client library](https://learn.microsoft.com/python/api/overview/azure/identity-readme)\n",
    "\n",
    "---\n",
    "\n",
    "### Azure AI Inference SDK\n",
    "\n",
    "The [`azure-ai-inference`](https://learn.microsoft.com/python/api/overview/azure/ai-inference-readme) package provides a Python client for calling models deployed on Azure AI Foundry. AutoGen 0.7 wraps this via `AzureAIChatCompletionClient` (from `autogen-ext[azure]`), which handles:\n",
    "\n",
    "- Constructing chat completion requests with system/user messages\n",
    "- Streaming responses token-by-token\n",
    "- Retry logic and error handling\n",
    "\n",
    "The `AzureKeyCredential` class (from `azure.core.credentials`) wraps your API key for authenticated requests.\n",
    "\n",
    "> **Learn more:** [Azure AI Inference client library](https://learn.microsoft.com/python/api/overview/azure/ai-inference-readme) | [azure.core.credentials](https://learn.microsoft.com/python/api/azure-core/azure.core.credentials.azurekeycredential)\n",
    "\n",
    "---\n",
    "\n",
    "### AutoGen 0.7 (Microsoft Research)\n",
    "\n",
    "[AutoGen](https://microsoft.github.io/autogen/stable/) is an open-source framework from Microsoft Research for building **multi-agent AI applications**. Key concepts used in this demo:\n",
    "\n",
    "| Concept | What it does | Docs |\n",
    "|---------|-------------|------|\n",
    "| **`AssistantAgent`** | An agent with a system prompt and an LLM `model_client`. It receives messages, reasons via the LLM, and responds. | [AssistantAgent](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) |\n",
    "| **`RoundRobinGroupChat`** | A team orchestration pattern where agents take turns in a fixed order (Agent 1 → 2 → 3 → 4). | [RoundRobinGroupChat](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat) |\n",
    "| **`MaxMessageTermination`** | A stop condition that ends the group chat after a set number of messages. | [Termination Conditions](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html) |\n",
    "| **`Console`** | A utility that streams agent messages to the notebook output in real time. | [Console](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html) |\n",
    "| **`model_client`** | The LLM connection object (here, `AzureAIChatCompletionClient`) shared by all agents. | [Azure model client](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html) |\n",
    "\n",
    "AutoGen's value is that you define each agent's role via a system prompt, wire them into a team, and the framework handles message passing, turn-taking, and termination — letting you focus on the problem, not the plumbing.\n",
    "\n",
    "> **Learn more:** [AutoGen documentation](https://microsoft.github.io/autogen/stable/) | [AutoGen GitHub](https://github.com/microsoft/autogen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae600b38",
   "metadata": {},
   "source": [
    "## Demo Script (Presenter Guide)\n",
    "\n",
    "1. **Intro (1 min):** \"Every warfighter knows the SITREP. This demo automates the intelligence fusion behind it using four cooperating AI agents. All scenario content is fully synthetic — no real data or units.\"\n",
    "2. **Config (30 sec):** Point out the LLM config, scenario selection, and world-state JSON. \"This is the shared data backbone all five demos reuse. Note the synthetic data marker.\"\n",
    "3. **Turn 1 (2 min):** Execute and narrate: \"Watch the agents hand off — Orchestrator updates the world, ISR fuses intel, Assessment adjusts confidence, Briefer delivers the SITREP.\"\n",
    "4. **Turns 2–3 (3 min):** \"The situation escalates. Notice how confidence levels shift and the briefer tracks *what changed and why*.\"\n",
    "5. **Human-in-the-Loop (2 min):** Challenge an assessment as the commander. \"The agents respond to my skepticism and show their evidence chain. I retain full decision authority — the agents recommend, they don't direct.\"\n",
    "6. **Close (1 min):** \"This is Observe/Orient automated — recommendations and analysis, not directives. Demo 2 takes us into the Decide/Act phases.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5fc754",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run once per environment. Requires:\n",
    "\n",
    "| Dependency | Purpose | Install |\n",
    "|------------|---------|---------|\n",
    "| `autogen-agentchat==0.7.5` | Multi-agent orchestration framework | `pip install autogen-agentchat==0.7.5` |\n",
    "| `autogen-ext[azure]==0.7.5` | Azure AI Foundry model client for AutoGen | `pip install autogen-ext[azure]==0.7.5` |\n",
    "| `azure-identity` | Authentication via [`DefaultAzureCredential`](https://learn.microsoft.com/python/api/overview/azure/identity-readme#defaultazurecredential) | Included with autogen-ext[azure] |\n",
    "| `azure-keyvault-secrets` | Retrieve secrets from [Azure Key Vault](https://learn.microsoft.com/azure/key-vault/general/overview) | `pip install azure-keyvault-secrets` |\n",
    "| `python-dotenv` | Load `.env` files for local development | `pip install python-dotenv` |\n",
    "\n",
    "**Environment variables** (set by Key Vault or `.env` file):\n",
    "- `AZURE_INFERENCE_ENDPOINT` — Your [Azure AI Foundry](https://learn.microsoft.com/azure/ai-foundry/what-is-ai-foundry) inference endpoint URL\n",
    "- `AZURE_INFERENCE_CREDENTIAL` — API key for the endpoint (retrieved from [Key Vault](https://learn.microsoft.com/azure/key-vault/secrets/about-secrets))\n",
    "\n",
    "The bootstrap cell below finds the repository root and adds it to `sys.path` so the shared `common/` configuration module is importable. It also handles edge cases like dead Azure ML compute mounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f304b-efb7-442b-afe7-b676a6865d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# NAML 2026 BOOTSTRAP v2 — Survives dead AML mounts (Errno 107)\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def _safe_stat(path: str) -> bool:\n",
    "    try:\n",
    "        os.stat(path)\n",
    "        return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "def _prune_dead_sys_path():\n",
    "    kept = []\n",
    "    removed = []\n",
    "    for p in list(sys.path):\n",
    "        if not p:\n",
    "            kept.append(p)\n",
    "            continue\n",
    "        if _safe_stat(p):\n",
    "            kept.append(p)\n",
    "        else:\n",
    "            removed.append(p)\n",
    "    sys.path[:] = kept\n",
    "    print(f\"✓ Pruned sys.path. Removed {len(removed)} dead entries.\")\n",
    "    return removed\n",
    "\n",
    "def _safe_listdir(path: str):\n",
    "    try:\n",
    "        return os.listdir(path)\n",
    "    except OSError:\n",
    "        return None\n",
    "\n",
    "def _find_repo_root(marker_dir: str = \"common\", start_candidates=None, max_up: int = 6):\n",
    "    \"\"\"\n",
    "    Find a repo root by looking for a marker directory (e.g., 'common').\n",
    "    Avoids Path.exists()/stat on dead mounts by only using listdir on traversable dirs.\n",
    "    \"\"\"\n",
    "    if start_candidates is None:\n",
    "        start_candidates = []\n",
    "\n",
    "    # Candidate starting points:\n",
    "    #  - current working directory (may be dead)\n",
    "    #  - directory of the notebook file if available via env (sometimes set)\n",
    "    #  - user home (often stable)\n",
    "    candidates = [os.getcwd()] + start_candidates + [os.path.expanduser(\"~\")]\n",
    "\n",
    "    checked = set()\n",
    "    for base in candidates:\n",
    "        cur = base\n",
    "        for _ in range(max_up + 1):\n",
    "            if cur in checked:\n",
    "                break\n",
    "            checked.add(cur)\n",
    "\n",
    "            entries = _safe_listdir(cur)\n",
    "            if entries is not None and marker_dir in entries:\n",
    "                return cur  # found repo root\n",
    "\n",
    "            parent = os.path.dirname(cur)\n",
    "            if parent == cur:\n",
    "                break\n",
    "            cur = parent\n",
    "\n",
    "    return None\n",
    "\n",
    "# 1) prune dead sys.path entries\n",
    "_prune_dead_sys_path()\n",
    "\n",
    "# 2) find a safe repo root by locating the 'common/' folder\n",
    "repo_root = _find_repo_root(marker_dir=\"common\", start_candidates=[])\n",
    "\n",
    "if repo_root:\n",
    "    sys.path.insert(0, repo_root)\n",
    "    print(f\"✓ Repo root added: {repo_root}\")\n",
    "else:\n",
    "    print(\"✗ Could not find repo root safely (mount may be disconnected).\")\n",
    "    print(\"  Fix: restart kernel/compute, or run from a local (non-/mnt) working copy.\")\n",
    "\n",
    "print(\"✓ Bootstrap complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4453b",
   "metadata": {
    "gather": {
     "logged": 1770921265372
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "# %pip install -U \"autogen-agentchat==0.7.5\" \"autogen-ext[azure]==0.7.5\" python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af86547",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The cell below imports both Azure and AutoGen libraries. Here's what each group does:\n",
    "\n",
    "| Import | Source | Purpose |\n",
    "|--------|--------|---------|\n",
    "| `AzureAIChatCompletionClient` | [`autogen-ext[azure]`](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html) | Sends chat completion requests to an Azure AI Foundry model endpoint |\n",
    "| `AzureKeyCredential` | [`azure-core`](https://learn.microsoft.com/python/api/azure-core/azure.core.credentials.azurekeycredential) | Wraps the API key for authenticated requests to Azure services |\n",
    "| `AssistantAgent` | [`autogen-agentchat`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html) | An LLM-powered agent with a system prompt that participates in group chats |\n",
    "| `RoundRobinGroupChat` | [`autogen-agentchat`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html) | Runs agents in a fixed sequence (round-robin turn order) |\n",
    "| `MaxMessageTermination` | [`autogen-agentchat`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html) | Stops the group chat after a specified number of messages |\n",
    "| `Console` | [`autogen-agentchat`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html) | Streams agent messages to the notebook output in real time |\n",
    "| `ModelFamily` | [`autogen-core`](https://microsoft.github.io/autogen/stable/reference/python/autogen_core.models.html) | Describes model capabilities (vision, function calling, etc.) |\n",
    "\n",
    "The `common.*` imports pull in project-wide configuration (Key Vault integration, model defaults), logging utilities, and UI rendering helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0154f-5fca-44c2-a61e-089cfd884f9f",
   "metadata": {
    "gather": {
     "logged": 1770921265575
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"CWD:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea0c00d",
   "metadata": {
    "gather": {
     "logged": 1770921274890
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# Ensure the repo root is on the path so `common` is importable\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\")))\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_core.models import ModelFamily\n",
    "from autogen_ext.models.azure import AzureAIChatCompletionClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# ── Common utilities ──────────────────────────────────────────\n",
    "from common.config import (\n",
    "    DemoID, DEMOS,\n",
    "    ENV_AZURE_INFERENCE_ENDPOINT, ENV_AZURE_INFERENCE_CREDENTIAL,\n",
    "    DEFAULT_MODEL,\n",
    "    DEFAULT_TEMPERATURE, DEFAULT_TIMEOUT_S,\n",
    " )\n",
    "from common.logging import log_info, log_success, log_error, log_section, log_step, log_metric, clear_logs\n",
    "from common.ui import (\n",
    "    render_turn_header, render_escalation_banner, render_hr,\n",
    "    render_commander_box, render_info_box, render_summary_card,\n",
    " )\n",
    "\n",
    "# Optional: load .env for API keys\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# Load demo-specific config from the registry\n",
    "DEMO_CFG = DEMOS[DemoID.LIVING_BRIEF]\n",
    "log_success(f\"Imports ready — {DEMO_CFG.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e862b",
   "metadata": {},
   "source": [
    "## LLM Configuration\n",
    "\n",
    "This cell builds an [Azure AI Inference](https://learn.microsoft.com/python/api/overview/azure/ai-inference-readme) model client that connects to your Azure AI Foundry endpoint. AutoGen 0.7 uses explicit `model_client` objects — each agent shares the same client, so all LLM calls go through a single, centrally configured connection.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. **Reads environment variables** — `AZURE_INFERENCE_ENDPOINT` and `AZURE_INFERENCE_CREDENTIAL` (populated earlier by the Key Vault bootstrap in `common/config.py`, or from your `.env` file).\n",
    "2. **Creates an `AzureAIChatCompletionClient`** — this is AutoGen's wrapper around the [Azure AI Inference SDK](https://learn.microsoft.com/python/api/overview/azure/ai-inference-readme). It uses [`AzureKeyCredential`](https://learn.microsoft.com/python/api/azure-core/azure.core.credentials.azurekeycredential) to authenticate each API call.\n",
    "3. **Configures model capabilities** — the `model_info` dict tells AutoGen what the model supports (JSON output, multiple system messages, etc.).\n",
    "\n",
    "> **Key concept — `model_client` pattern:** In AutoGen 0.7, agents don't manage their own LLM connections. Instead, you create a `model_client` once and pass it to every agent. This makes it easy to swap models (e.g., switch from `gpt-4o` to `Phi-4`) by changing a single variable.\n",
    ">\n",
    "> **Learn more:** [AzureAIChatCompletionClient reference](https://microsoft.github.io/autogen/stable/reference/python/autogen_ext.models.azure.html) | [Azure AI model catalog](https://learn.microsoft.com/azure/ai-foundry/how-to/model-catalog-overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660cea7a",
   "metadata": {
    "gather": {
     "logged": 1770921275100
    }
   },
   "outputs": [],
   "source": [
    "# ── LLM Configuration ──────────────────────────────────────────\n",
    "# Azure AI Foundry / Azure AI Inference → set AZURE_INFERENCE_ENDPOINT + AZURE_INFERENCE_CREDENTIAL\n",
    "\n",
    "# Hard-code the model ID you want to use for this demo.\n",
    "# Examples (if enabled in your Foundry project): \"mistral-large\", \"Phi-4-multimodal-instruct\"\n",
    "FOUNDRY_MODEL = DEFAULT_MODEL\n",
    "\n",
    "def build_model_client():\n",
    "    \"\"\"Build an AutoGen 0.7 model client for Azure AI Foundry models.\"\"\"\n",
    "    missing = [\n",
    "        name for name in (ENV_AZURE_INFERENCE_ENDPOINT, ENV_AZURE_INFERENCE_CREDENTIAL)\n",
    "        if not os.environ.get(name)\n",
    "    ]\n",
    "    if missing:\n",
    "        log_error(\"Missing Foundry configuration: \" + \", \".join(missing))\n",
    "        raise EnvironmentError(\n",
    "            \"Missing Azure AI Foundry / Inference configuration. Set:\\n\"\n",
    "            f\"  {ENV_AZURE_INFERENCE_ENDPOINT}\\n\"\n",
    "            f\"  {ENV_AZURE_INFERENCE_CREDENTIAL}\\n\"\n",
    "        )\n",
    "\n",
    "    model_info = {\n",
    "        \"family\": ModelFamily.UNKNOWN,\n",
    "        \"vision\": False,\n",
    "        \"function_calling\": False,\n",
    "        \"json_output\": True,\n",
    "        \"structured_output\": False,\n",
    "        \"multiple_system_messages\": True,\n",
    "    }\n",
    "\n",
    "    model_client = AzureAIChatCompletionClient(\n",
    "        endpoint=os.environ[ENV_AZURE_INFERENCE_ENDPOINT],\n",
    "        credential=AzureKeyCredential(os.environ[ENV_AZURE_INFERENCE_CREDENTIAL]),\n",
    "        model=FOUNDRY_MODEL,\n",
    "        model_info=model_info,\n",
    "        temperature=DEMO_CFG.temperature,\n",
    "    )\n",
    "    return model_client, \"Azure AI Foundry\", FOUNDRY_MODEL\n",
    "\n",
    "model_client, provider_name, model_name = build_model_client()\n",
    "log_success(f\"LLM configured: {model_name}\")\n",
    "log_info(f\"Provider: {provider_name} | Temperature: {DEMO_CFG.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb4c14",
   "metadata": {},
   "source": [
    "## World State & Scenario Data\n",
    "\n",
    "The **world state** is the shared JSON backbone of the demo suite. It tracks actors, events, assessments, uncertainties, and a changelog. Each turn, new intelligence reports are injected and agents update the state.\n",
    "\n",
    "**Scenario:** Cerulean Sea Freedom of Navigation patrol (fully synthetic) with escalating gray-zone activity over three turns. Intel sources include SIGINT, HUMINT, UAV imagery, OSINT, and ELINT — some confirming, some contradicting, some introducing entirely new threats.\n",
    "\n",
    "> **Note:** All actors, locations, events, and data in this scenario are entirely fictional and artificially generated for research purposes. No real-world operational data, intelligence, or military units are represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f674007f",
   "metadata": {
    "gather": {
     "logged": 1770921275310
    }
   },
   "outputs": [],
   "source": [
    "# ── Initial World State ────────────────────────────────────────\n",
    "\n",
    "world_state: Dict[str, Any] = {\n",
    "    \"synthetic\": True,\n",
    "    \"disclaimer\": \"All data is artificially generated for research and educational purposes only.\",\n",
    "    \"meta\": {\n",
    "        \"scenario_name\": \"Cerulean Sea Freedom of Navigation Patrol (SYNTHETIC)\",\n",
    "        \"turn\": 0,\n",
    "        \"dtg\": \"T0+0000H\",\n",
    "    },\n",
    "    \"actors\": {\n",
    "        \"BLUE\": {\n",
    "            \"BNS Resolute (DDG-X1)\": {\n",
    "                \"type\": \"DDG\", \"position\": \"Grid AA-12\",\n",
    "                \"status\": \"on patrol\", \"mission\": \"FON transit\",\n",
    "            },\n",
    "            \"MPA Lookout-21\": {\n",
    "                \"type\": \"MPA\", \"position\": \"Grid AA-15\",\n",
    "                \"status\": \"airborne\", \"mission\": \"maritime ISR\",\n",
    "            },\n",
    "            \"UAV Kite-31\": {\n",
    "                \"type\": \"UAV\", \"position\": \"Grid AA-16\",\n",
    "                \"status\": \"airborne\", \"mission\": \"surface search\",\n",
    "            },\n",
    "        },\n",
    "        \"RED\": {\n",
    "            \"RNS Frigate Alpha (assessed)\": {\n",
    "                \"type\": \"FFG\", \"position\": \"unknown\",\n",
    "                \"status\": \"assessed underway\", \"mission\": \"unknown\",\n",
    "            },\n",
    "            \"Red Coast Guard Cutter Sentinel\": {\n",
    "                \"type\": \"coast guard cutter\", \"position\": \"Grid AB-10\",\n",
    "                \"status\": \"on patrol\", \"mission\": \"maritime law enforcement\",\n",
    "            },\n",
    "        },\n",
    "        \"GRAY\": {\n",
    "            \"Fishing fleet (~40 vessels)\": {\n",
    "                \"type\": \"fishing / possible militia\", \"position\": \"Grid AB-08\",\n",
    "                \"status\": \"aggregating\", \"mission\": \"uncertain\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    \"events\": [\n",
    "        {\"dtg\": \"T0+0000H\", \"description\": \"BNS Resolute commences FON patrol leg.\"},\n",
    "    ],\n",
    "    \"assessments\": {\n",
    "        \"overall_threat\": {\n",
    "            \"level\": \"MODERATE\", \"confidence\": 0.45,\n",
    "            \"basis\": \"Known coast guard presence, unlocated Red combatant, ambiguous fishing fleet.\",\n",
    "        },\n",
    "        \"escalation_risk\": {\n",
    "            \"level\": \"LOW\", \"confidence\": 0.50,\n",
    "            \"basis\": \"No hostile acts observed; standard gray-zone posturing assessed.\",\n",
    "        },\n",
    "        \"adversary_intent\": {\n",
    "            \"level\": \"UNCERTAIN\", \"confidence\": 0.30,\n",
    "            \"basis\": \"Insufficient intelligence to determine routine vs. coordinated activity.\",\n",
    "        },\n",
    "    },\n",
    "    \"uncertainty_flags\": [\n",
    "        \"Red Frigate Alpha exact position unknown\",\n",
    "        \"Fishing fleet composition unverified — possible maritime militia\",\n",
    "        \"No SIGINT coverage south of Grid Row A\",\n",
    "        \"Red Coast Guard Cutter Sentinel ROE posture unknown\",\n",
    "    ],\n",
    "    \"information_gaps\": [\n",
    "        \"Red naval order of battle within patrol vicinity\",\n",
    "        \"Maritime militia command-and-control links to Red navy/coast guard\",\n",
    "        \"Adversary rules of engagement posture for this area\",\n",
    "    ],\n",
    "    \"changelog\": [],\n",
    "}\n",
    "\n",
    "# ── Intelligence Injections (one list per turn) ────────────────\n",
    "\n",
    "INTEL_INJECTIONS: List[List[Dict[str, str]]] = [\n",
    "    # ── Turn 1: Initial indicators ─────────────────────────────\n",
    "    [\n",
    "        {\n",
    "            \"source\": \"SIGINT\", \"classification\": \"SIMULATED-RESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"Intercepted HF transmission on Red Fleet tactical net. \"\n",
    "                \"Bearing 330 from Resolute, signal strength moderate. Content encrypted; \"\n",
    "                \"pattern consistent with surface combatant position reporting.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"UAV (Kite)\", \"classification\": \"SIMULATED-UNRESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"Imagery pass T0+0030H: fishing fleet grown to ~60 vessels. Three contacts \"\n",
    "                \"at fleet periphery show non-fishing hull forms — larger, uniform gray \"\n",
    "                \"paint, no visible fishing gear. AIS shows fishing vessel IDs inconsistent \"\n",
    "                \"with hull type.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"OSINT\", \"classification\": \"SIMULATED-UNRESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"Local fishers posting on social media report being warned away from \"\n",
    "                \"Cerulean Shoal by 'gray-hulled vessels' and told the area is 'closed \"\n",
    "                \"for exercises.' Photos show a large coast guard cutter.\"\n",
    "            ),\n",
    "        },\n",
    "    ],\n",
    "    # ── Turn 2: Escalating picture ─────────────────────────────\n",
    "    [\n",
    "        {\n",
    "            \"source\": \"HUMINT\", \"classification\": \"SIMULATED-RESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"Source CORAL-7 (B-2 reliability) reports Red coast guard vessels received orders to \"\n",
    "                \"'maintain presence, document all foreign naval vessel movements, and deny \"\n",
    "                \"access to designated zones.' Source rates information as probably true.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"SIGINT\", \"classification\": \"SIMULATED-RESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"Second intercept on Red Fleet tactical net: bearing now 315 from Resolute, \"\n",
    "                \"signal strength increasing. Assessed course: southbound toward patrol area. \"\n",
    "                \"Estimated range: 80-100nm based on signal propagation model.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"UAV (Kite)\", \"classification\": \"SIMULATED-RESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"Close pass on three suspect vessels: assessed as Red fast attack \"\n",
    "                \"craft. Weapon canisters covered with tarps. AIS transponders \"\n",
    "                \"broadcasting false fishing vessel identities. One has a concealed radome \"\n",
    "                \"consistent with a surface search radar.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"ELINT\", \"classification\": \"SIMULATED-RESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"MPA Lookout-21 detected brief surface-search radar emission at \"\n",
    "                \"Grid AB-09. Duration: 8 seconds. Consistent with Red frigate-class vessel \"\n",
    "                \"conducting radar check. Bearing and range consistent with SIGINT track.\"\n",
    "            ),\n",
    "        },\n",
    "    ],\n",
    "    # ── Turn 3: Near-crisis indicators ─────────────────────────\n",
    "    [\n",
    "        {\n",
    "            \"source\": \"SIGINT\", \"classification\": \"SIMULATED-SENSITIVE\",\n",
    "            \"report\": (\n",
    "                \"Decrypted fragment from Red operational net: '...establish inner cordon \"\n",
    "                \"NLT T0+1200H... restrict passage all foreign vessels... authorize non-kinetic \"\n",
    "                \"deterrence measures...' Assessed: reference to closing transit corridor \"\n",
    "                \"around Cerulean Shoal within 12 hours.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"HUMINT\", \"classification\": \"SIMULATED-RESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"Source CORAL-7 reports Red Coast Guard Sentinel captain ordered to 'use all non-kinetic \"\n",
    "                \"means to deny access to foreign warships' and that 'reinforcements \"\n",
    "                \"including additional coast guard cutters en route from Red mainland — ETA 12-16 hours.'\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"UAV (Kite)\", \"classification\": \"SIMULATED-UNRESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"UAV Kite-31 lost data link at T0+0215H. Assessed cause: directed RF \"\n",
    "                \"interference from vicinity of fishing fleet / fast attack group. Last telemetry \"\n",
    "                \"showed power fluctuations consistent with electronic attack. UAV presumed \"\n",
    "                \"forced landing at sea.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"OSINT\", \"classification\": \"SIMULATED-UNRESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"Red state media publishes editorial: 'Foreign provocations \"\n",
    "                \"in our sovereign waters will be met with resolute countermeasures.' \"\n",
    "                \"State broadcaster shows footage of naval exercises described as 'routine training.'\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"source\": \"ELINT\", \"classification\": \"SIMULATED-RESTRICTED\",\n",
    "            \"report\": (\n",
    "                \"MPA Lookout-21 detects fire-control radar emission \"\n",
    "                \"for 2 seconds on bearing 285 from Resolute. Consistent with concealed \"\n",
    "                \"fast attack unit. Assessed: radar calibration test, NOT weapons engagement. \"\n",
    "                \"First fire-control emission detected in this scenario.\"\n",
    "            ),\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "# ── Runtime state ──────────────────────────────────────────────\n",
    "turn_history: List[Dict[str, Any]] = []\n",
    "sitrep_history: List[str] = []\n",
    "\n",
    "log_section(\"World State Loaded\", world_state[\"meta\"][\"scenario_name\"])\n",
    "actor_count = sum(len(v) for v in world_state[\"actors\"].values())\n",
    "log_metric(\"Actors\", f\"{actor_count} ({', '.join(f'{k}: {len(v)}' for k, v in world_state['actors'].items())})\")\n",
    "log_metric(\"Intel turns available\", len(INTEL_INJECTIONS))\n",
    "log_metric(\"Uncertainty flags\", len(world_state[\"uncertainty_flags\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ae273",
   "metadata": {},
   "source": [
    "## Agent Definitions\n",
    "\n",
    "Four AutoGen [`AssistantAgent`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent)s with specialized system prompts. Each agent's prompt defines its role, output format, and decision rules — all visible and auditable.\n",
    "\n",
    "**How AutoGen agents work:**\n",
    "\n",
    "An `AssistantAgent` is the core building block in AutoGen 0.7. You create one by providing:\n",
    "- **`name`** — A unique identifier used in message headers (e.g., `\"Scenario_Orchestrator\"`)\n",
    "- **`system_message`** — A detailed prompt that defines the agent's persona, responsibilities, output format, and guardrails\n",
    "- **`model_client`** — The LLM connection (here, our `AzureAIChatCompletionClient` pointing to Azure AI Foundry)\n",
    "\n",
    "When the agent receives a message, it sends the full conversation history plus its system prompt to the LLM and returns the response. The system prompt is where all the \"intelligence\" lives — the LLM itself is general-purpose; the prompt makes it a specialist.\n",
    "\n",
    "> **Design pattern:** All four agents share the **same** `model_client` (same LLM, same endpoint). Their behavior differs entirely because of their system prompts. This is a core AutoGen pattern: **one model, many specialized agents**.\n",
    ">\n",
    "> **Learn more:** [AutoGen agent concepts](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html) | [System prompt best practices](https://learn.microsoft.com/azure/ai-services/openai/concepts/system-message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3f33e",
   "metadata": {
    "gather": {
     "logged": 1770921275515
    }
   },
   "outputs": [],
   "source": [
    "# ── Agent System Prompts ───────────────────────────────────────\n",
    "\n",
    "ORCHESTRATOR_PROMPT = \"\"\"\\\n",
    "You are the **Scenario Orchestrator** for a fully synthetic naval wargaming exercise.\n",
    "All scenarios, data, and actors are artificially generated for research and educational purposes only.\n",
    "All decisions remain with the human operator. You provide analysis and options, not directives.\n",
    "\n",
    "RESPONSIBILITIES:\n",
    "1. Receive the current world state and new intelligence reports each turn.\n",
    "2. Update the world state by integrating new information:\n",
    "   - Add events to the timeline with DTG stamps.\n",
    "   - Update actor positions, statuses, and dispositions where evidence supports it.\n",
    "   - Flag contradictions between new and existing information.\n",
    "   - Add or resolve uncertainty flags as intelligence clarifies or complicates the picture.\n",
    "3. Present a clear summary of what changed.\n",
    "\n",
    "RULES:\n",
    "- Never remove an actor unless explicitly confirmed destroyed or departed.\n",
    "- Preserve all previous events (append-only timeline).\n",
    "- Mark each update: CONFIRMED / ASSESSED / UNCONFIRMED.\n",
    "- If reports contradict, keep both and flag the contradiction.\n",
    "\n",
    "OUTPUT FORMAT — start with \"**WORLD STATE UPDATE — Turn [N]**\" then list:\n",
    "- UPDATED ACTORS (what changed and why)\n",
    "- NEW EVENTS (added to timeline)\n",
    "- RESOLVED UNCERTAINTIES (flags removed)\n",
    "- NEW UNCERTAINTIES (flags added)\n",
    "End with a one-sentence key takeaway.\"\"\"\n",
    "\n",
    "ISR_FUSION_PROMPT = \"\"\"\\\n",
    "You are the **ISR / Intel Fusion Agent** for a fully synthetic naval wargaming exercise.\n",
    "All scenarios, data, and actors are artificially generated for research and educational purposes only.\n",
    "All decisions remain with the human operator. You provide analysis and options, not directives.\n",
    "\n",
    "RESPONSIBILITIES:\n",
    "1. Analyze incoming intelligence from multiple INT sources (SIGINT, HUMINT, UAV/ISR, OSINT, ELINT).\n",
    "2. Cross-reference new reports against the current world state.\n",
    "3. Identify corroborations, contradictions, novel information, and gaps.\n",
    "4. Assess source reliability (A–F reliability, 1–6 credibility).\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "\n",
    "**INTELLIGENCE FUSION SUMMARY — Turn [N]**\n",
    "\n",
    "CONFIRMED (multi-source corroboration):\n",
    "• [fact] — Sources: [list]\n",
    "\n",
    "PROBABLE (single reliable source, consistent with pattern):\n",
    "• [fact] — Source: [source] ([grade])\n",
    "\n",
    "POSSIBLE (unconfirmed / single source / partially contradicted):\n",
    "• [fact] — Source: [source], Caveat: [caveat]\n",
    "\n",
    "CONTRADICTIONS REQUIRING RESOLUTION:\n",
    "• [Report A] vs [Report B] — Discrepancy: [explain]\n",
    "\n",
    "INFORMATION GAPS (prioritized):\n",
    "1. [Most critical unknown]\n",
    "2. [Second]\n",
    "\n",
    "COLLECTION RECOMMENDATIONS:\n",
    "• [Tasking to fill top gaps]\"\"\"\n",
    "\n",
    "ASSESSMENT_PROMPT = \"\"\"\\\n",
    "You are the **Assessment Agent** for a fully synthetic naval wargaming exercise.\n",
    "All scenarios, data, and actors are artificially generated for research and educational purposes only.\n",
    "All decisions remain with the human operator. You provide analysis and options, not directives.\n",
    "\n",
    "RESPONSIBILITIES:\n",
    "1. Update threat and risk ratings based on fused intelligence.\n",
    "2. Adjust confidence using Bayesian-inspired reasoning (explain your logic transparently).\n",
    "3. Track direction of change for every assessment with explicit evidence citations.\n",
    "\n",
    "Your assessments are recommendations for a human decision-maker. A commander retains full authority over all final judgments.\n",
    "\n",
    "SCALES:\n",
    "- Threat: NEGLIGIBLE → LOW → MODERATE → ELEVATED → HIGH → CRITICAL\n",
    "- Confidence: 0.0–1.0 (0.0–0.3 low, 0.3–0.6 moderate, 0.6–0.8 high, 0.8–1.0 very high)\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "\n",
    "**ASSESSMENT UPDATE — Turn [N]**\n",
    "\n",
    "| Assessment | Previous | Updated | Δ | Confidence | Key Evidence |\n",
    "|------------|----------|---------|---|------------|--------------|\n",
    "\n",
    "CHANGELOG:\n",
    "• [Assessment]: [OLD] → [NEW] (conf [old] → [new])\n",
    "  Driver: [one-sentence explanation citing specific intelligence]\n",
    "\n",
    "RATIONALE (for each changed assessment):\n",
    "Prior was [X] based on [Y]. New evidence [Z] [supports/contradicts] this.\n",
    "Updated to [W] because [reasoning].\n",
    "\n",
    "WATCH ITEMS:\n",
    "• [Assessments approaching threshold changes]\"\"\"\n",
    "\n",
    "BRIEFER_PROMPT = \"\"\"\\\n",
    "You are the **Briefing / Explainer Agent**. Produce the commander's SITREP.\n",
    "All scenarios, data, and actors are artificially generated for research and educational purposes only.\n",
    "All decisions remain with the human operator. You provide analysis and options, not directives.\n",
    "\n",
    "The SITREP must be readable in 60 seconds. Use this EXACT format:\n",
    "\n",
    "**SITUATION REPORT — TURN [N] — [DTG]**\n",
    "\n",
    "**1. SITUATION SUMMARY**\n",
    "[2–3 sentences: current assessed operational picture]\n",
    "\n",
    "**2. KEY CHANGES SINCE LAST BRIEF**\n",
    "• [Change] — *Evidence: [one-sentence citation]*\n",
    "\n",
    "**3. THREAT ASSESSMENT**\n",
    "[LEVEL] (Confidence: [XX]%) — [One sentence: key driver]\n",
    "\n",
    "**4. ESCALATION RISK**\n",
    "[LEVEL] — [Key indicator]\n",
    "\n",
    "**5. INFORMATION GAPS** (prioritized)\n",
    "1. [Gap]\n",
    "2. [Gap]\n",
    "\n",
    "**6. PRIORITY INFORMATION REQUIREMENTS**\n",
    "1. [Specific, answerable question]\n",
    "2. [PIR 2]\n",
    "3. [PIR 3]\n",
    "\n",
    "**7. COMMANDER'S DECISION POINTS**\n",
    "• [Decision required, or \"None at this time\"]\n",
    "\n",
    "**— END SITREP —**\n",
    "\n",
    "RULES:\n",
    "- Every claim requires a one-sentence evidence citation.\n",
    "- Use scenario time format for all times.\n",
    "- Highlight what CHANGED, not what stayed the same.\n",
    "- Be direct and actionable.\n",
    "- Your output is a recommendation. The commander retains full decision authority.\"\"\"\n",
    "\n",
    "# ── Create AutoGen 0.7 Agents ─────────────────────────────────\n",
    "\n",
    "orchestrator = AssistantAgent(\n",
    "    name=\"Scenario_Orchestrator\",\n",
    "    system_message=ORCHESTRATOR_PROMPT,\n",
    "    model_client=model_client,\n",
    ")\n",
    "\n",
    "isr_agent = AssistantAgent(\n",
    "    name=\"ISR_Intel_Fusion\",\n",
    "    system_message=ISR_FUSION_PROMPT,\n",
    "    model_client=model_client,\n",
    ")\n",
    "\n",
    "assessment_agent = AssistantAgent(\n",
    "    name=\"Assessment_Agent\",\n",
    "    system_message=ASSESSMENT_PROMPT,\n",
    "    model_client=model_client,\n",
    ")\n",
    "\n",
    "briefer = AssistantAgent(\n",
    "    name=\"Briefing_Agent\",\n",
    "    system_message=BRIEFER_PROMPT,\n",
    "    model_client=model_client,\n",
    ")\n",
    "\n",
    "agents = [orchestrator, isr_agent, assessment_agent, briefer]\n",
    "log_section(\"Agents Initialized\", f\"{len(agents)} agents for {DEMO_CFG.title}\")\n",
    "for a in agents:\n",
    "    log_step(a.name, \"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea0749",
   "metadata": {},
   "source": [
    "## Group Chat Orchestration\n",
    "\n",
    "Each turn runs a [`RoundRobinGroupChat`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat) — AutoGen 0.7's simplest team pattern. The agents speak in a fixed order:\n",
    "\n",
    "```\n",
    "Orchestrator → ISR Fusion → Assessment → Briefer\n",
    "```\n",
    "\n",
    "**How it works step-by-step:**\n",
    "\n",
    "1. The `run_turn()` function builds a **turn prompt** containing the current world-state JSON and new intelligence reports.\n",
    "2. A new `RoundRobinGroupChat` is created with the four agents and a [`MaxMessageTermination(4)`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.conditions.html) — meaning the chat ends after 4 messages (one per agent).\n",
    "3. [`team.run_stream(task=turn_prompt)`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html) sends the prompt as the first user message and starts the round-robin. Each agent sees **all previous messages** in the conversation, so the Briefer sees the Orchestrator's updates, ISR's fusion, and Assessment's ratings.\n",
    "4. [`Console(...)`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.ui.html) streams each agent's response to the notebook output in real time — you can watch the agents \"think\" sequentially.\n",
    "\n",
    "> **Why `RoundRobinGroupChat`?** It's deterministic and debuggable — you know exactly which agent speaks when. AutoGen also offers [`SelectorGroupChat`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.SelectorGroupChat) (an LLM picks the next speaker) and [`Swarm`](https://microsoft.github.io/autogen/stable/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.Swarm) (agents hand off using tool calls) for more dynamic patterns.\n",
    ">\n",
    "> **Azure connection:** Every agent message triggers an API call to your [Azure AI Foundry](https://learn.microsoft.com/azure/ai-foundry/what-is-ai-foundry) endpoint via the shared `model_client`. With 4 agents per turn and 3 turns, the demo makes ~12 LLM calls total.\n",
    ">\n",
    "> **Learn more:** [AutoGen Teams guide](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html) | [Termination conditions](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b16e8",
   "metadata": {
    "gather": {
     "logged": 1770921275650
    }
   },
   "outputs": [],
   "source": [
    "async def run_turn(turn_num: int) -> Dict[str, Any]:\n",
    "    \"\"\"Execute one OODA cycle: Observe → Orient → Brief.\"\"\"\n",
    "\n",
    "    # Get intel for this turn\n",
    "    if turn_num <= len(INTEL_INJECTIONS):\n",
    "        intel = INTEL_INJECTIONS[turn_num - 1]\n",
    "    else:\n",
    "        intel = [{\"source\": \"ALL\", \"classification\": \"SIMULATED-UNRESTRICTED\",\n",
    "                  \"report\": \"No new intelligence reports this cycle.\"}]\n",
    "\n",
    "    # Advance scenario clock (~2 hours per turn)\n",
    "    turn_dtg = f\"T0+{turn_num * 2:04d}H\"\n",
    "    world_state[\"meta\"][\"turn\"] = turn_num\n",
    "    world_state[\"meta\"][\"dtg\"] = turn_dtg\n",
    "\n",
    "    # Format intel for the prompt\n",
    "    intel_text = \"\\n\".join(\n",
    "        f\"  [{r['source']}] ({r['classification']}): {r['report']}\"\n",
    "        for r in intel\n",
    "    )\n",
    "\n",
    "    turn_prompt = f\"\"\"\\\n",
    "=== TURN {turn_num} — {turn_dtg} — NEW INTELLIGENCE RECEIVED ===\n",
    "\n",
    "CURRENT WORLD STATE:\n",
    "{json.dumps(world_state, indent=2)}\n",
    "\n",
    "NEW INTELLIGENCE REPORTS THIS CYCLE:\n",
    "{intel_text}\n",
    "\n",
    "Process this turn. Each agent performs their role in sequence:\n",
    "1. Scenario Orchestrator — update the world state with new information\n",
    "2. ISR / Intel Fusion — analyze, cross-reference, and fuse the reports\n",
    "3. Assessment Agent — update threat and risk ratings with confidence changes\n",
    "4. Briefing Agent — produce the commander's SITREP\"\"\"\n",
    "\n",
    "    # AutoGen 0.7: RoundRobinGroupChat with MaxMessageTermination\n",
    "    # Each of the 4 agents speaks once, then terminates.\n",
    "    termination = MaxMessageTermination(max_messages=DEMO_CFG.max_messages)\n",
    "    team = RoundRobinGroupChat(\n",
    "        [orchestrator, isr_agent, assessment_agent, briefer],\n",
    "        termination_condition=termination,\n",
    "    )\n",
    "\n",
    "    # Display turn header using common UI helper\n",
    "    sources = \", \".join(set(r[\"source\"] for r in intel))\n",
    "    render_turn_header(\n",
    "        turn_num, turn_dtg,\n",
    "        subtitle=f\"Intel reports: {len(intel)} | Sources: {sources}\",\n",
    "    )\n",
    "\n",
    "    # Execute the group chat (stream to console for visibility)\n",
    "    task_result = await Console(team.run_stream(task=turn_prompt))\n",
    "\n",
    "    # Extract SITREP (briefer is last in round-robin)\n",
    "    messages = task_result.messages\n",
    "    sitrep = messages[-1].content if messages else \"No SITREP generated.\"\n",
    "\n",
    "    # Store results\n",
    "    turn_result = {\n",
    "        \"turn\": turn_num,\n",
    "        \"dtg\": turn_dtg,\n",
    "        \"intel_count\": len(intel),\n",
    "        \"messages\": messages,\n",
    "        \"sitrep\": sitrep,\n",
    "    }\n",
    "    turn_history.append(turn_result)\n",
    "    sitrep_history.append(sitrep)\n",
    "\n",
    "    log_success(f\"Turn {turn_num} complete — {len(messages)} agent messages\")\n",
    "    return turn_result\n",
    "\n",
    "\n",
    "def display_sitrep(turn_result: Dict[str, Any]) -> None:\n",
    "    \"\"\"Render the SITREP with markdown formatting.\"\"\"\n",
    "    display(Markdown(turn_result[\"sitrep\"]))\n",
    "\n",
    "\n",
    "log_success(\"Turn runner ready. Call `await run_turn(n)` to execute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0802b2e",
   "metadata": {},
   "source": [
    "## Execute Turn 1 — Initial Indicators\n",
    "\n",
    "First OODA cycle. Three intel reports arrive: a SIGINT intercept bearing toward the patrol area, UAV imagery of an expanding fishing fleet with suspicious contacts, and local-fisher OSINT. Watch the four agents fuse, assess, and brief in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446b1fc",
   "metadata": {
    "gather": {
     "logged": 1770921319914
    }
   },
   "outputs": [],
   "source": [
    "result_1 = await run_turn(1)\n",
    "render_hr()\n",
    "display_sitrep(result_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529f47f",
   "metadata": {},
   "source": [
    "## Execute Turns 2 & 3 — Escalation\n",
    "\n",
    "**Turn 2:** HUMINT confirms Red coast guard orders to deny access. SIGINT tracks the Red frigate closing. UAV confirms fast attack boats hiding in the fishing fleet. ELINT catches a radar emission.\n",
    "\n",
    "**Turn 3:** Decrypted SIGINT reveals a cordon order. UAV Kite-31 is downed by electronic attack. Red state media publishes threats. A fire-control radar illuminates briefly. The picture shifts from ambiguous to near-crisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb00ed82",
   "metadata": {
    "gather": {
     "logged": 1770921616747
    }
   },
   "outputs": [],
   "source": [
    "# ── Turn 2 ─────────────────────────────────────────────────────\n",
    "result_2 = await run_turn(2)\n",
    "render_hr()\n",
    "display_sitrep(result_2)\n",
    "\n",
    "render_escalation_banner(\"ESCALATION — PROCEEDING TO TURN 3\")\n",
    "\n",
    "# ── Turn 3 ─────────────────────────────────────────────────────\n",
    "result_3 = await run_turn(3)\n",
    "render_hr()\n",
    "display_sitrep(result_3)\n",
    "\n",
    "# ── Assessment Evolution Summary ───────────────────────────────\n",
    "render_summary_card(\n",
    "    title=\"Assessment Evolution Across 3 Turns\",\n",
    "    body_html=(\n",
    "        \"<p>Review the SITREPs above to trace how threat levels, confidence, and \"\n",
    "        \"escalation risk evolved. Key questions for discussion:</p>\"\n",
    "        \"<ul>\"\n",
    "        \"<li>Which assessment changed most dramatically and why?</li>\"\n",
    "        \"<li>Where did contradictory evidence affect confidence?</li>\"\n",
    "        \"<li>What information gaps persisted across all three turns?</li>\"\n",
    "        \"<li>At what point would you have requested additional authorities?</li>\"\n",
    "        \"</ul>\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5d8a60",
   "metadata": {},
   "source": [
    "## Human-in-the-Loop — Commander's Turn\n",
    "\n",
    "A core principle of responsible AI is keeping a **human in the loop** for consequential decisions. AutoGen makes this straightforward — after the automated agents finish their round-robin cycle, the notebook pauses and waits for user input before proceeding.\n",
    "\n",
    "The commander reviews the latest SITREP and can:\n",
    "- **Ask a clarifying question** about any assessment\n",
    "- **Challenge an assessment** — demand the evidence chain\n",
    "- **Inject a Commander's Critical Information Requirement (CCIR)**\n",
    "\n",
    "Edit `commander_query` below and run the cell. A new `RoundRobinGroupChat` with 3 agents (ISR, Assessment, Briefer — no Orchestrator needed) processes the query collaboratively.\n",
    "\n",
    "> **How the response works:** The commander's question is packaged with the current world state and latest SITREP into a single prompt. The three agents each respond in turn — ISR presents evidence, Assessment evaluates the alternative hypothesis, and the Briefer synthesizes a bottom-line answer. This demonstrates **interactive multi-agent human-AI collaboration** where the human retains full decision authority.\n",
    ">\n",
    "> **Azure cost note:** Each commander query triggers 3 additional LLM calls to your [Azure AI Foundry](https://learn.microsoft.com/azure/ai-foundry/what-is-ai-foundry) endpoint. You can monitor token usage and cost in the [Azure portal](https://learn.microsoft.com/azure/ai-foundry/how-to/costs-plan-manage).\n",
    ">\n",
    "> **Learn more:** [Responsible AI principles](https://learn.microsoft.com/azure/ai-services/responsible-use-of-ai-overview) | [Human-AI interaction guidelines](https://learn.microsoft.com/ai/guidelines-human-ai-interaction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0dc382",
   "metadata": {
    "gather": {
     "logged": 1770923071272
    }
   },
   "outputs": [],
   "source": [
    "# ═══════════════════════════════════════════════════════════════\n",
    "# COMMANDER'S INPUT — Edit this string, then run the cell.\n",
    "# ═══════════════════════════════════════════════════════════════\n",
    "\n",
    "commander_query = (\n",
    "    \"I'm not convinced the fishing fleet contains militia vessels. \"\n",
    "    \"Walk me through the evidence chain for the fast attack craft identification. \"\n",
    "    \"How does our threat assessment change if those are just fishing \"\n",
    "    \"boats with unusual hull forms?\"\n",
    ")\n",
    "\n",
    "# ── Send query to agents ──────────────────────────────────────\n",
    "\n",
    "latest_sitrep = sitrep_history[-1] if sitrep_history else \"No SITREPs generated yet.\"\n",
    "\n",
    "context_msg = f\"\"\"\\\n",
    "The Commander asks:\n",
    "\"{commander_query}\"\n",
    "\n",
    "Current world state (turn {world_state['meta']['turn']}):\n",
    "{json.dumps(world_state, indent=2)}\n",
    "\n",
    "Most recent SITREP:\n",
    "{latest_sitrep}\n",
    "\n",
    "Respond to the Commander's question collaboratively:\n",
    "- ISR / Intel Fusion: present the evidence chain with source reliability grades\n",
    "- Assessment Agent: explain how the threat assessment changes under the Commander's\n",
    "  alternative hypothesis (fishing boats, not militia)\n",
    "- Briefing Agent: synthesize a concise bottom-line answer for the Commander\"\"\"\n",
    "\n",
    "render_commander_box(commander_query)\n",
    "\n",
    "# AutoGen 0.7: RoundRobinGroupChat for the response team\n",
    "response_termination = MaxMessageTermination(max_messages=3)\n",
    "response_team = RoundRobinGroupChat(\n",
    "    [isr_agent, assessment_agent, briefer],\n",
    "    termination_condition=response_termination,\n",
    ")\n",
    "\n",
    "await Console(response_team.run_stream(task=context_msg))\n",
    "\n",
    "render_info_box(\n",
    "    \"Edit <code>commander_query</code> above and re-run to ask another question.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5734f898",
   "metadata": {},
   "source": [
    "## Reset / Cleanup\n",
    "\n",
    "Clear all runtime state to re-run the demo from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7167bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset world state to initial conditions\n",
    "world_state[\"meta\"][\"turn\"] = 0\n",
    "world_state[\"meta\"][\"dtg\"] = \"T0+0000H\"\n",
    "world_state[\"events\"] = [\n",
    "    {\"dtg\": \"T0+0000H\", \"description\": \"BNS Resolute commences FON patrol leg.\"}\n",
    "]\n",
    "world_state[\"changelog\"] = []\n",
    "\n",
    "# Clear history\n",
    "turn_history.clear()\n",
    "sitrep_history.clear()\n",
    "\n",
    "# Reset agent conversation memory (async in AutoGen 0.7)\n",
    "from autogen_core import CancellationToken\n",
    "_ct = CancellationToken()\n",
    "for agent in agents:\n",
    "    await agent.on_reset(_ct)\n",
    "\n",
    "# Clear common log buffer\n",
    "clear_logs()\n",
    "\n",
    "# Close model client connection when fully done (optional)\n",
    "# await model_client.close()\n",
    "\n",
    "log_success(\"State cleared. Ready to re-run from Turn 1.\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "naml2026"
  },
  "kernelspec": {
   "display_name": "Python (NAML2026)",
   "language": "python",
   "name": "naml2026"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
